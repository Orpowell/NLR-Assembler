import csv
import logging
import pickle
import sys

from itertools import permutations
from collections import Counter

import click
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logging.basicConfig(stream=sys.stdout, format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',
                    level=logging.INFO)


def extract_mapping_data(sam_file, NLR_annotation):
    """
    The function takes a SAM file as an input and generates a dictionary of all the reads that have been mapped to a
    given refernce sequence in the SAM file as list.

    :param NLR_annotation:
    :param sam_file: reads mapped to contigs generated by assembly of reads
    :return: dictionary: {contig:[all mapped reads]}
    """
    contig_read_dictionary = {}
    logging.info("extracting infromation from SAM file...")
    with open(sam_file) as file:
        for line in file:
            # extract all reference sequence names from SAM file and add to dictionary with an empty list
            if line.startswith("@SQ"):
                seq_name = line.split("\t")[1][3:]
                contig_read_dictionary[seq_name] = []

            # skip the header added by samtools
            elif line.startswith("@PG"):
                continue

            # read line of SAM file and extract reference sequence and query sequence names then append query sequence
            # to a list in the dictionary using the reference sequence as a key
            else:
                read_info = line.split("\t")
                query_name = read_info[0]
                reference_name = read_info[2]
                # if a sequence is unmapped to a reference move onto the next read
                if reference_name == "*":
                    continue
                else:
                    contig_read_dictionary[reference_name].append(query_name)

    logging.info("extracting NLR annotaion data...")
    nlr_contigs = []

    with open(NLR_annotation) as annotations:
        for line in annotations:
            line_data = line.split("\t")
            nlr_contigs.append(line_data[0])

    nlr_read_dictionary = {nlr: contig_read_dictionary[nlr] for nlr in nlr_contigs}

    return nlr_read_dictionary


def convert_reads_to_hexidecimal(contig_read_dictionary, index):
    """
    The function takes a dictionary of contigs each with a list of reads mapped to that contig and converts the read
    names to a list of rgb values based on an index file. The list RGB values is then converted to a string of
    hexidecimal values that can later be analysed to compare the similarity of contigs.

    :param contig_read_dictionary: dictionary of reads mapped to each contig generated by extracting_mapping_data()
    :param index: csv file where each read has been assigned a colour based its adapter sequence
    :return: contig_hex_dicitonary: {contig: string of hexidecimal values assinged for each read}
    """
    logging.info("extracting infromation from index file...")
    with open(index, mode='r') as inp:
        index_reader = csv.reader(inp)
        ID_colour_dict = {rows[0]: rows[1] for rows in index_reader}

    logging.info("converting Seq IDs to RGB values...")
    contig_rgb = {contig: tuple(map(lambda x: ID_colour_dict[x], contig_read_dictionary[contig])) for contig in
                  contig_read_dictionary}

    logging.info("converting rgb values to hexidecimal...")
    contig_hex_dictionary = {
        contig: list(map(lambda x: '#%02x%02x%02x' % tuple(map(int, x.split(","))), contig_rgb[contig])) for
        contig in contig_rgb}

    return contig_hex_dictionary


def generate_cosine_matrix(contig_hex_dictionary):
    logging.info("converting hexidecimal to strings...")
    contig_adapter_profiles = [" ".join(contig_hex_dictionary[contig]) for contig in
                               contig_hex_dictionary]

    logging.info("calculating cosine similarity...")
    count_array = CountVectorizer()
    profile_count_array = count_array.fit_transform(contig_adapter_profiles)
    cosine_array = cosine_similarity(profile_count_array)

    return cosine_array


def group_contigs(contig_hex, cosine_array, threshold):
    def flatten(array):
        return [item for sublist in array for item in sublist]

    def filter_by_cosine(array, dictionary, filter_threshold):
        valid_contigs = []
        for n, cosine in enumerate(array):
            if cosine > filter_threshold:
                valid_contigs.append(dictionary[n])

        return sorted(valid_contigs)

    normalised_keys = {n: k for n, k in enumerate(list(contig_hex.keys()))}

    array_dict = {normalised_keys[n]: array for n, array in enumerate(cosine_array)}

    grouped_contigs = [filter_by_cosine(v, normalised_keys, threshold) for k, v in array_dict.items()]

    combos = permutations(grouped_contigs, 2)
    sub_groups = [g1 for g1, g2 in combos if set(g1).issubset(set(g2)) and g1 != g2]
    non_duplicate_group = [val.split() for val in
                           list(set([" ".join(val) for val in grouped_contigs if val not in sub_groups]))]

    bad_contigs = [k for k, v in Counter(flatten(non_duplicate_group)).items() if v > 1]

    for bc in bad_contigs:
        non_duplicate_group = [group for group in non_duplicate_group if bc not in group]

    for val in list(contig_hex.keys()):
        if val not in flatten(non_duplicate_group):
            non_duplicate_group.append([val])

    return non_duplicate_group


def find_optimal_grouping(cosine_threshold_dictionary):
    n_grouped_contigs = {str(k): len([contig for contig in v if len(contig) > 1]) for k, v in
                         cosine_threshold_dictionary.items()}
    x = n_grouped_contigs.keys()
    y = list(n_grouped_contigs.values())

    col = []
    optimal_threshold = max(y)
    for val in y:
        if val == optimal_threshold:
            col.append('red')
        else:
            col.append('blue')

    plt.bar(*zip(*n_grouped_contigs.items()), color=col)
    plt.xlabel('Cosine Similarity Threshold')
    plt.ylabel('# Grouped Contigs')

    for i in range(len(x)):
        plt.text(i, y[i] + 2.5, y[i], ha='center')

    plt.savefig('cosine_threshold.png', dpi=300)

    return max(n_grouped_contigs, key=n_grouped_contigs.get)

def pickle_data(contig_grouping):
    logging.info('saving raw contig groups to pickle...')
    with open("raw_cosine_grouping.pkl", 'wb') as file:
        pickle.dump(contig_grouping, file)


@click.command()
@click.option('-i', '--samfile', type=str, required=True, help="SAM file")
@click.option('-n', '--nlr', type=str, required=True, help="NLR annotator file")
@click.option('-x', '--index', type=str, required=True, help="Index file generated with colour mapper")
def calculate_similarity(samfile, nlr, index):
    nlr_contig_reads = extract_mapping_data(samfile, nlr)
    contig_hex = convert_reads_to_hexidecimal(nlr_contig_reads, index)
    cosine_matrix = generate_cosine_matrix(contig_hex)

    threshold_dictionary = {threshold: group_contigs(contig_hex, cosine_matrix, threshold) for threshold in
                            [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1]}

    optimal_threshold = find_optimal_grouping(threshold_dictionary)

    best_contig_grouping = threshold_dictionary[float(optimal_threshold)]

    pickle_data(best_contig_grouping)
